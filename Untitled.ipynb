{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randrange, random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_breast_cancer, load_iris, load_wine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wine data\n",
    "wine_data = load_wine()\n",
    "\n",
    "#put dataset into data frame\n",
    "X = pd.DataFrame(wine_data['data'], columns=wine_data.feature_names)\n",
    "y = pd.Series(wine_data.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  \n",
       "0                          3.92   1065.0  \n",
       "1                          3.40   1050.0  \n",
       "2                          3.17   1185.0  \n",
       "3                          3.45   1480.0  \n",
       "4                          2.93    735.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#three classes\n",
    "y.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into test/train\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9661016949152542"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DT_model = DecisionTreeClassifier()\n",
    "DT_model.fit(X_train.values, y_train)\n",
    "y_hat = DT_model.predict(X_test)\n",
    "accuracy_score(y_test, y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrapped_samp(x_train, y_train, ratio=1.0):\n",
    "    x_sample = []\n",
    "    y_sample = []\n",
    "    n = round(len(x_train) * ratio)\n",
    "    while len(x_sample) < n:\n",
    "        index = randrange(len(x_train))\n",
    "        x_sample.append(x_train[index])\n",
    "        y_sample.append(y_train[index])\n",
    "    return x_sample, y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trees = [5, 10, 15, 20, 35,30, 35, 40, 45, 50]\n",
    "final_accs = []\n",
    "for trees in n_trees:\n",
    "    accuracies = list()\n",
    "    for i in range(trees):\n",
    "        x_samps, y_samps = bootstrapped_samp(X_train.values, y_train.values)\n",
    "        DT_model = DecisionTreeClassifier()\n",
    "        DT_model.fit(x_samps, y_samps)\n",
    "        y_hat = DT_model.predict(X_test)\n",
    "        accuracies.append(accuracy_score(y_test, y_hat))\n",
    "        \n",
    "    final_accs.append(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_means = [np.mean(final_accs[i]) for i in range(len(n_trees))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.9118644067796611,\n",
       "  0.9118644067796609,\n",
       "  0.9423728813559322,\n",
       "  0.9220338983050846,\n",
       "  0.921549636803874,\n",
       "  0.9248587570621468,\n",
       "  0.9167070217917674,\n",
       "  0.925,\n",
       "  0.9133709981167606,\n",
       "  0.9186440677966102],\n",
       " [5, 10, 15, 20, 35, 30, 35, 40, 45, 50])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_means, n_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f69646b0650>]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(n_trees, avg_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blind approach\n",
    "\n",
    "As you've seen before, creating a model is the easy part. Try just using everything that you've got and throwing it without much thought into a random forest. The scikit-learn package requires the independent variables to be numeric, and all you want are dummy variables. So use the get_dummies() method from pandas to generate a dummy variable for every categorical column, and see what happens with this kind of naive approach. The get_dummies() method takes anything that is considered an object type and converts the value into a column in the DataFrame. The new column will contain a 1 if the value exists in that row and 0 if it does not exist.\n",
    "\n",
    "Below, use the grade column as an example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Success Criteria\n",
    "\n",
    "#### Today I will be successful if I can...\n",
    "1. Explain bagging in one or two sentences\n",
    "1. Express why Random Forests work better than traditional Bagging\n",
    "1. Name 3 of the hyper-parameters are in a random forest\n",
    "1. Express strengths and weaknesses of a Random Forest Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Methods\n",
    "\n",
    "Ensemble methods at it's heart is a combination of many weak predictor machine learning models to form a strong model. \n",
    "\n",
    "Example: Betting on the winning horse. (classification example)\n",
    "Guessing the number of jellybeans in the jar (regression example)\n",
    "The wisdom of the crowd produces a much better guess than the individual decision. \n",
    "For regression, avgerage the predictors\n",
    "For Classification, Plurality of choice or the average of the percentages for each class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "Decision trees are great, and their easily represented set of rules is a powerful feature for modeling—and even more so for conveying that model to a more general audience. But their high variance and propensity to overfit are serious problems. Luckily, there is a way that you can handle both of those issues and create an even more powerful kind of model.\n",
    "\n",
    "What if, instead of making one decision tree, you made several? As many as you wanted, really—a whole forest. And what if each tree in the forest got a vote on the outcome for a given observation? Then you'd have a new model type: random forest. Random forests have become an incredibly popular technique for data scientists, because this method tends to be a top performer with low variance and high accuracy in a huge number of circumstances.\n",
    "\n",
    "Much like decision trees, random forest can be used for both classification and regression problems. The main difference is how the votes are aggregated. As a classifier, the most popular outcome (the mode) is returned. And as a regression, it is typically the average (the mean) that is returned.\n",
    "\n",
    "- How is splitting determined?\n",
    "\n",
    "- What would be the difference between a decision tree trained with the same data?\n",
    "\n",
    "Since Decision Trees are deterministic, they will continously give the same tree over and over when given the same data, this is where we bring back Bootsrapping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap + Aggregating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a random forest?\n",
    "\n",
    "What if, instead of making one decision tree, you made several? As many as you wanted, really—a whole forest. And what if each tree in the forest got a vote on the outcome for a given observation? Then you'd have a new model type: random forest. Random forests have become an incredibly popular technique for data scientists, because this method tends to be a top performer with low variance and high accuracy in a huge number of circumstances.\n",
    "\n",
    "Much like decision trees, random forest can be used for both classification and regression problems. The main difference is how the votes are aggregated. As a classifier, the most popular outcome (the mode) is returned. And as a regression, it is typically the average (the mean) that is returned.\n",
    "\n",
    "\n",
    "## Parameters\n",
    "\n",
    "When building a random forest, you get to set parameters for both the tree and the forest. So for the tree, you have the same parameters as before: you can set the depth of the tree and the number of features used in each rule or split. You can also specify how the tree is built; you can use information gain and entropy like you did before, or you can use other methods, like [Gini impurity](https://www.garysieling.com/blog/sklearn-gini-vs-entropy-criteria).\n",
    "\n",
    "You also get to control the number of estimators that you want to generate, or the number of trees in the forest. Here you have a tradeoff between how much variance you can explain and the computational complexity. This is pretty easily tunable. As you increase the number of trees in the forest, the accuracy should converge; eventually, the additional learning from another tree approaches zero. There isn't an infinite amount of information to learn; at some point, the trees have learned all they can. So when you have an acceptable variance in accuracy, you can stop adding trees. This becomes worthwhile when you're dealing with large datasets with many variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADVANTAGES\n",
    "\n",
    "    One of the most accurate learning algorithms available\n",
    "    It can handle many predictor variables\n",
    "    Provides estimates of the importance of different predictor variables\n",
    "    Maintains accuracy even when a large proportion of the data is missing\n",
    "\n",
    "LIMITATIONS\n",
    "\n",
    "    Can overfit datasets that are particularly noisy\n",
    "    For data including categorical predictor variables with different number of levels, random forests are biased in favor of those predictors with more levels. Therefore, the variable importance scores from random forest are not always reliable for this type of data\n",
    "\n",
    "\n",
    "## Advantages and disadvantages of random forest\n",
    "\n",
    "The biggest advantage of random forest is its tendency to be a very strong performer. It is reasonably accurate in a myriad of situations, from regression to classification. Some people [really love random forests](https://medium.com/rants-on-machine-learning/the-unreasonable-effectiveness-of-random-forests-f33c3ce28883#.rq8akkff1). However, it does have some disadvantages.\n",
    "\n",
    "Firstly, in both classification and regression, it will not predict outside of the sample. This means that it will only return values that are within a range that it has seen before. Random forests can also get rather large and slow if you let them grow too wildly.\n",
    "\n",
    "The biggest disadvantage, however, is the lack of transparency in the process. Random forest is often referred to as a *black-box model*; it provides an output but very little insight into how it got there. You'll run into a few more of these black-box models throughout the program.\n",
    "\n",
    "Black-box models often make the more statistically minded data scientists nervous. You don't get much insight into the process. You can't see the rules that it's really applying, or what variables it's prioritizing, or how. You don't see any of the internal processes, and you don't get to look \"inside the box.\" Therefore, you also can't represent that process in a simple visual form or learn about the underlying process. You have to trust in the algorithm building the trees and the lack of variance from a large number of them being generated. It usually works out pretty well, and you can of course evaluate the model via other methods to validate your conclusions.\n",
    "\n",
    "In the next section, you'll walk through an example of the random forest classifier.\n",
    "\n",
    "\n",
    "ASSUMPTIONS\n",
    "\n",
    "No formal distributional assumptions, random forests are non-parametric and can thus handle skewed and multi-modal data as well as categorical data that are ordinal or non-ordinal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
